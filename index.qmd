---
talk-title: " MESSI-benchmark"
talk-subtitle: "MESSI: Multiomics Experiments with SyStematic Interrogation"
talk-descrip: "A Nextflow pipeline for benchmarking multiomics integration methods"
author: "Chunqing (Tony) Liang"
other-authors: "Dr. Amrit Singh"
talk-date: "March 20, 2025"
format: revealjs
---



## {#title background-image="assets/website_bkg.png" background-position="top"}

<!--- setup r lib -->
```{r}
#| label: "setup"
#| echo: false
#| include: false
#| eval: true

library(knitr)
library(cowplot)
```



<br>
<br>
<br>
<br>
<br>
<br>


### {{< meta talk-subtitle >}} 

::: {.f2}

{{< meta talk-descrip >}}

:::

<br>

<br>


::: {.f3 .bold}

{{< meta author >}} 

PhD student in Bioinformatics

Supervisor: {{< meta other-authors >}}

{{< meta talk-date >}}

:::

<!-- {{< include _titleslide.qmd >}} -->


## Multiomics data and analysis

:::: {.columns}

::: {.column width="48%"}

```{r}
#| label: "fig-multiomics-intro"
#| fig-align: center
#| fig-cap-location: margin
#| fig-cap: "               Overview of multiomics data and integration [@shannon2024commentary]"
knitr::include_graphics("assets/multi-omics-integration.png")
```

:::

::: {.column width="52%"}

<br>

<br>

::: {.callout-tip}

# Motivation

- Technological advancement and reduced costs --> studies with multiomics data

- Multiomics data --> /single-cell/spatial data

- More generalized --> "multimodal data", other non-omics data

- Types of integrations: by sample (N), by omics (P), or both

:::

:::

::::

::: {.notes}

- Recent update in tech --> cheap and easy access of multiomics data
- Which has different types
- Moreover, we now consider modalities , not just omics but images, doctor notes, so on
- Now, how to integrate them, simplest is same N different P, N means subjects P being predictors

:::


## Multiomics data integration methods

:::: {.columns}

::: {.column width="45%"}

```{r out.width="600px", out.height="480px"}
#| label: "fig-integration-methods"
#| fig-align: center
#| fig-cap-location: bottom
#| fig-cap: "Types of multiomics integration method [@subramanian2020multi]"
knitr::include_graphics("assets/methods_avail.jpg")
```

:::
<!--- Empty column here --->
::: {.column width="5%"} 
<!--- Empty column here --->
:::


::: {.column width="50%"}

<br>

::: {.callout-note .fragment}

# Question arises

- Many integration methods 
  
  ---> Which to use, how to choose them?

- [Reproducibility crisis]{.secondary}
  
  ---> How to reproduce method and get [reliable]{.tertiary} results?

- Existing benchmark studies are not 100% complete or all-encompassing

  ---> Technical difficulty in implementation?

:::

:::

::::

::: {.notes}

- Earlier talk about data, but which way we know could maximize / make use of full potential from data
- Then, which task are we performing, and how do you know if it is better in your data not just by chance?
- Fast way to validate them? Hard to reproduce, not really work on custom data
- Have anyone try setting this validation up? 
  - Reliable? Covered all? Easy to access? 

:::


## The MESSI workflow

*Multiomics Experiments with SyStematic Interrogation*, **MESSI**

:::: {.columns}

::: {.column width="45%"}

```{r}
#| label: "fig-messi-overview-new"
#| fig-align: center
#| fig-cap: "Overview of MESSI workflow"
knitr::include_graphics("assets/messi_workflow.png")
```

:::

::: {.column width="2%"}
:::

::: {.column width="53%"}

- This is done in [Nextflow]{.primary} [@di2017nextflow], and will be publicly available in **nf-core** [@ewels2020nf].

- Compare large set of data at once

- Solves [reproducibility]{.secondary} issue through independent method containers

- Generalizable for more integration methods, without affecting existing workflows

- Standardized data format transferrable between `MultiAssayExperiment` and `MuData`

:::

::::

::: {.notes}

- Workflow is this, gather your data in MAE and MuData, these two are transferrable
  - Requires a binary response here in the data, and just numeric data
- Perform classification task here, could simply extend to regression and other
- Split data prior to perfrom stratified CV
  - Not all methods have built-in CV
  - And accounts for imbalanced data

- The data could make multiple copies to feed into many method workflow possible
  - 1 method is single short pipeline consist of its own prepro, train, and pred
  - Gather predicted probabilities of P(Y=1), or positive case
  - Each method has its own container --> independet from every other method
  - Due to independence, possible to parallel all, at extreme method-data-fold 

- Through this we ensure reproducibility, same environment, same seed of splitting, make sure same data flow
- Adding or editing or deleting method do not alter rest


:::


## Methods

- Looking into multiview [@ding2022cooperative], DIABLO [@singh2019diablo], RGCCA [@girka2023multiblock], MOFA [@argelaguet2018multi], MOGONET [@wang2021mogonet] 
- Using a **5-fold** cross validation (cv) with [AUC]{.secondary} score as metric
- Validating with known ground truth data from **500+** simulated data with varying parameters
  - "Signal" to distinguish groups (binary response) 
  - [Correlation]{.tertiary} between omics
- Evaluating on **6** real world data ([@fig-real-tbl])


```{r}
#| label: "fig-real-tbl"
#| fig-align: center
#| fig-cap: "Real world data demographics"

#knitr::include_graphics("assets/computed_results/new_table.png")
knitr::include_graphics("assets/computed_results/real_table.png")
```


::: {.notes}

- To verify pipeline, we run on 5 integration methods, high impact journal, see reference

- As mentioned focused on classification task and have pred prob as output from pipeline, so use auc as metric
  - Set different threshold could yield diff output --> so use auc
- auc measures modelâ€™s ability to assign higher predicted probabilities to the positive class than to the negative class. 
  - Higher (close to 1) able to tell which is positive / negative
  - Lower (close to 0) struggles to differentiate
  - 0.5 model do not learn, random guessing

- Now, we need ground truth, have code to simulate multiomics data
  - Given choices and number of parameters we have 500+ unique data
  - most important have a signal to distinguish group (response)
  - correlation between omics (covariance)
  - ~~Could improve then~~

- Lastly on some real world data, high dim, study different disesase or condition

- R-based one are usually not deep learning based, this is of bad GPU support and focus more on statistics

:::



## Method are sensitive to the signal in the data

:::: {.columns}

::: {.column width="47%"}

```{r}
#| label: "fig-perf-sim"
#| fig-align: center
#| fig-cap: "5-fold CV AUC on simulated data with varying correlation and signal"
knitr::include_graphics("assets/computed_results/fig_performance_evaluation_sim.png")
```

:::

::: {.column width="2%"}

:::

::: {.column width="51%"} 

<br>

- **Higher signal**, better performance, since it is easier for methods to predict the response

- Correlation does not have obvious effect on performance, additional test required

- [DIABLO]{.primary} is the best, with marginal difference to mofa + glmnet

- [MOGONET]{.quaternary} is the worst despite being deep learning-based

- Methods perform quite well even when given little signal, with AUC score of ~ **0.7**

:::

::::

::: {.notes}

- No signal, hard to tell, all equally fail and end at 0.5
- Little signal --> quickly to 0.7, to some point it reaches close to 1
- This obvious, since now we make response i.e. pos and neg to have very distinct values
- Corr do not have very obvious effect, arguably lower performance
  - As high corr between omics, redundant info, hence not good?
- Bit small, but seems diablo full or null design matrix (covariance matrix of omics) do good
- Some see suffix is because original model do not perform classification, so some tricks
- MOGONET is bad in almost all, deep learning is not always good

:::

## High variability when identifying the important predictors

:::: {.columns}

::: {.column width="49%"}

```{r}
#| label: "fig-feat-sim"
#| fig-align: center
#| fig-cap: "Sensitivity of methods identifying important predictors"
knitr::include_graphics("assets/computed_results/fig_feature_selection_sim.png")
```

:::

::: {.column width="1%"}
:::

::: {.column width="50%"}

<br>

<!-- - [Multiview]{.primary} have consistenly identifying important predictors even when **low** signal or correlation
- Rest methods seemed to have variety of sensitivity with different parameter settings, [unstable]{.tertiary}
- [MOGONET]{.quaternary} performs worst, have constantly low median of sensitivity capped ~ 0.1 -->


- [Multiview]{.primary} has consistently identified important predictors, even when the signal or correlation is **low**.
- The other methods exhibited varying sensitivity with different parameter settings, making them [unstable]{.tertiary}.
- [MOGONET]{.quaternary} performed the worst, with low median sensitivity capped at around 0.1.

:::

::::

::: {.notes}

- Guide people, let them laugh a bit wow result not always good looking
- Since simulated, we know which predictor are useful (biomarker), which are garbage just noise, can model still pick the good ones? at what prop -> sensitivity
- vice versa, we could extend it to specificity and so on, lot of possibilities
- if you like to call stable, mogonet consistent bad at 0.1, but pink multiview is doing great at lesat stable
- diablo best model before still do good jobm but sometimes just fails a identifying biomarkers 

- Discuss later that simulation could be improved or explored more

:::


## No universal method that work well on all datasets

:::: {.columns}

::: {.column width="46%"}

```{r}
#| label: "fig-perf-real"
#| fig-align: center
#| fig-cap: "5-fold CV AUC score on real data"
knitr::include_graphics("assets/computed_results/fig_performance_evaluation_real.png")
```

:::

::: {.column width="2%"}
:::

::: {.column width="52%"}


- All methods are unique in their design --- **"no method work well on all data"**
- Some data exhibits very [complex]{.secondary} patterns, making them challenging to predict
- Some methods are similar to each other, i.e. solving similar optimization problem, model setup
  - Similarly, data could have alike distributions
- The pattern [follows]{.tertiary} from previous simulation

:::

::::

::: {.notes}

- When comes to real, we dont see extreme settings like in simulation
- Turns out they perform quite good with 0.7 most , this kinda follows like before
  - Given signal, we have 0.7 as well wow, that why we use simulation first
- Now, some data is just complex, brca, all do poorly
- Looking at clusters, methods relate model setup, data relate distribution

:::

## Ranking of biomarkers varies by method


:::: {.columns}

::: {.column width="46%"}

```{r}
#| label: "fig-feat-real"
#| fig-align: center
#| fig-cap: "Spearman ranking correlation of biomarkers in methods"
knitr::include_graphics("assets/computed_results/fig_feature_selection_real.png")
```

:::

::: {.column width="1%"}
:::

::: {.column width="53%"}

<br>

- Methods do not use same metric to quantify whether certain biomarker is important, so **ranks** on these are usually [different]{.secondary}
  - Some methods relate due to similar model design like [DIABLO]{.tertiary} and [SGCCA]{.tertiary}
- Similar data tend to cluster together, i.e. `GSE71669` with `tcga-blca`, both study bladder cancer

:::

::::

::: {.notes}

- In here, we do not know what are actual useful biomarker, so instead let methods weights all predictors and rank them
  - important ones should be picked up and rank higher than those useless one
  - can we see similarity there? --> spearman rank correlation

- Due to diff method nature, we dont expect them to rank same, hope they all retrieve up important ones
  - if not means method need improvement? we could use pipeline to access multiple method
  - hence telling if your method need work

- SGCCA coincides with diablo-null, since both use canonical correlation analysis, to maximize correlation /covariance  between linear combination of variables nad projects them in lower dimension

:::

<!--------

UNCOMMENT THIS BELOW for 15 min talk instead

----->

<!-- <!---- -->

## {#Computational-time .smaller}

### Computational time is stationary regardless of dataset size


```{r}
#| label: fig-comp-time
#| fig-cap: "Computational time of methods from preprocessing to evaluation"
#| out-width: 77%
knitr::include_graphics("assets/computed_results/new_comp_time.png")

```

- Computational time is more sensitive to number of predictors (p) rather than observations (n)


## Discussion & Future directions {.smaller}

- "**No method works universally well on all datasets**"
- Classic statistical methods still work, and sometimes even better than deep learning (DL)
- Pipeline proves way to [reproducibly]{.tertiary} explore, benchmark different aspects of integration methods
  - Resumable
  - Parallel to compute as many resources as allowed at the same time
  - Ease burden of [setting up environment]{.secondary}

- Need to add more methods and datasets
  - Especially DL models are more popular now
  - Explore if any dataset could have relation to another despite different disease/condition

::: {.callout-note .fragment .bigger}

# Collaboration is welcomed!

This is ongoing work, and it would be great to have more community involvement

:::

::: {.notes}

- Resumable means internet, resource constrain, runs out of battery, we could recover right away and continue

- Close up on pipeline, quite ongoing work
- Add more data, for developers to evaluate their method on all these data
- Add more method, for users to simply validate it on their custom data without need to setup constantly
- Hopefully benefits community

:::


# Thanks! {.thanks}

## {#acknowledgements .smaller}



### Acknowledgements

:::: {.columns}

::: {.column width="50%"}


- [Dr. Amrit Singh]{.bold}
- Dr. Young Woong Kim
- Dr. Maryam Ahmadzadeh
- Rishika Daswani
- Roy He
- Michael Yoon
- Jeffrey Tang
- Akshdeep Sandhu
- Yovindu Don
- Raam Sivakumar
- Prabhleen Sandhu
- Mingming Zhang
- Samuel Leung

:::
::: {.column width="50%"}

```{r}
knitr::include_graphics("assets/lab_pic.jpg")
```

:::

::::


::: {.footer}
![](assets/logo.png)
:::

## References

::: {#refs}
:::
